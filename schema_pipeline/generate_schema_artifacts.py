"""Generate Python models and metadata from the CRM JSON schema.

This script ingests ``data/fake_crm_tables_schema.json`` (or a user-supplied
path) and emits reproducible artifacts under ``schema_pipeline/generated`` so
every downstream generator/judge class shares the same source of truth.

Artifacts:
    - ``schema_metadata.json``: normalized schema dictionary
    - ``schema_models.py``: Pydantic models + helper metadata constants
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List, Tuple


PYTHON_TYPE_MAP = {
    "string": "str",
    "number": "float",
    "integer": "int",
    "boolean": "bool",
    "object": "Dict[str, Any]",
}


def _python_type(property_spec: Dict[str, Any]) -> Tuple[str, List[str]]:
    """Return python annotation + required imports for a given JSON Schema prop."""
    prop_type = property_spec.get("type", "string")
    fmt = property_spec.get("format")
    enum = property_spec.get("enum")
    imports: List[str] = []

    if enum:
        options = ", ".join(f'"{value}"' for value in enum)
        imports.append("Literal")
        return f"Literal[{options}]", imports

    if prop_type == "array":
        items = property_spec.get("items", {})
        inner_type, inner_imports = _python_type(items)
        imports.extend(inner_imports)
        imports.append("List")
        return f"List[{inner_type}]", imports

    if fmt == "date":
        imports.append("date")
        return "date", imports
    if fmt == "date-time":
        imports.append("datetime")
        return "datetime", imports
    if fmt == "email":
        imports.append("EmailStr")
        return "EmailStr", imports
    if fmt == "uri":
        imports.append("AnyUrl")
        return "AnyUrl", imports

    return PYTHON_TYPE_MAP.get(prop_type, "Any"), imports


def _load_schema(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def _normalize_schema(raw_schema: Dict[str, Any]) -> Dict[str, Any]:
    tables = raw_schema.get("properties", {})
    normalized = {}
    for name, table in tables.items():
        properties = {}
        for field_name, spec in (table.get("properties") or {}).items():
            properties[field_name] = {
                key: spec.get(key)
                for key in ("type", "description", "enum", "format", "items", "minimum", "maximum")
                if key in spec
            }
        normalized[name] = {
            "description": table.get("description", ""),
            "required": table.get("required", []),
            "properties": properties,
        }
    return {"tables": normalized}


def _generate_models_py(metadata: Dict[str, Any], output_path: Path) -> None:
    future_imports = {"from __future__ import annotations"}
    typing_names = {"Any", "Dict"}
    datetime_names: set[str] = set()
    pydantic_names = {"BaseModel", "Field"}

    lines: List[str] = [
        '"""Autogenerated CRM schema models. Do not edit by hand."""',
        "",
    ]

    for table_name, table_schema in metadata["tables"].items():
        required = set(table_schema.get("required", []))
        class_lines = [f"class {table_name}(BaseModel):", f'    """{table_schema.get("description", "").strip()}"""' or "    \"\"\"\"\"\""]
        for field_name, spec in table_schema["properties"].items():
            python_type, needed_imports = _python_type(spec)
            for imp in needed_imports:
                if imp in {"date", "datetime"}:
                    datetime_names.add(imp)
                elif imp in {"Literal", "List", "Optional"}:
                    typing_names.add(imp)
                elif imp == "EmailStr":
                    pydantic_names.add("EmailStr")
                elif imp == "AnyUrl":
                    pydantic_names.add("AnyUrl")

            annotation = python_type
            default = "..."
            if field_name not in required:
                typing_names.add("Optional")
                annotation = f"Optional[{python_type}]"
                default = "None"
            description = spec.get("description")
            doc = f', description="{description}"' if description else ""
            class_lines.append(f"    {field_name}: {annotation} = Field({default}{doc})")
        lines.extend(class_lines)
        lines.append("")

    lines.append("TABLE_SCHEMAS: Dict[str, Any] = " + repr(metadata["tables"]))
    lines.append("")

    header_lines: List[str] = sorted(future_imports)
    if datetime_names:
        header_lines.append(f"from datetime import {', '.join(sorted(datetime_names))}")
    if typing_names:
        header_lines.append(f"from typing import {', '.join(sorted(typing_names))}")
    if pydantic_names:
        header_lines.append(f"from pydantic import {', '.join(sorted(pydantic_names))}")

    header_lines.append("")

    output_content = "\n".join(header_lines + lines)
    output_path.write_text(output_content, encoding="utf-8")


def _write_metadata(metadata: Dict[str, Any], path: Path) -> None:
    path.write_text(json.dumps(metadata, indent=2, sort_keys=True), encoding="utf-8")


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate schema artifacts for the CRM pipeline.")
    parser.add_argument("--schema-path", type=Path, default=Path("data/fake_crm_tables_schema.json"))
    parser.add_argument("--output-dir", type=Path, default=Path("schema_pipeline/generated"))
    args = parser.parse_args()

    raw_schema = _load_schema(args.schema_path)
    metadata = _normalize_schema(raw_schema)
    args.output_dir.mkdir(parents=True, exist_ok=True)
    _write_metadata(metadata, args.output_dir / "schema_metadata.json")
    _generate_models_py(metadata, args.output_dir / "schema_models.py")
    print(f"Wrote schema artifacts to {args.output_dir}")


if __name__ == "__main__":
    main()
